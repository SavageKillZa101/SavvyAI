<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Low-End AI Chat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body{font-family: system-ui, sans-serif; max-width: 700px; margin: 2rem auto; padding: 1rem;}
    #chat{ border: 1px solid #ccc; height: 60vh; overflow-y: auto; padding: 1rem; margin-bottom: 1rem; background:#fafafa;}
    .msg{margin:0.5rem 0;}
    .user{color:#d00; font-weight:bold;}
    .ai{color:#00a;}
    textarea{width:100%; box-sizing:border-box; font-family:inherit;}
    button{margin-top:0.5rem; padding:0.5rem 1rem;}
    #status{font-size:0.8rem; color:#666;}
  </style>
</head>
<body>

<h1>Low-End AI Chat</h1>
<div id="chat"></div>
<textarea id="input" rows="3" placeholder="Ask anything..."></textarea><br/>
<button onclick="send()">Send</button>
<div id="status">Choose mode →</div>

<script>
  // ==== CONFIG ==== 
  const MODE = "ollama";   // "ollama" or "webllm"

  // Ollama (change only if you run on non-standard port)
  const OLLAMA_URL = "http://localhost:11434/api/chat";

  // WebLLM model (hosted on HF – no key needed)
  const WEBLLM_MODEL = "onnx-community/Llama-3.2-1B-Instruct-q4f16";

  // ==== END CONFIG ====

  const chat = document.getElementById("chat");
  const input = document.getElementById("input");
  const status = document.getElementById("status");

  let webllmEngine = null;

  async function initWebLLM() {
    status.textContent = "Downloading model (~400 MB)...";
    const { pipeline } = await import("https://cdn.jsdelivr.net/npm/@huggingface/transformers@3");
    webllmEngine = await pipeline("text-generation", WEBLLM_MODEL, {
      dtype: "q4f16",
      device: "webgpu" in navigator ? "webgpu" : "wasm"
    });
    status.textContent = "Model ready (browser)";
  }

  function addMsg(role, text) {
    const div = document.createElement("div");
    div.className = "msg " + role;
    div.textContent = (role==="user"?"You":"AI") + ": " + text;
    chat.appendChild(div);
    chat.scrollTop = chat.scrollHeight;
  }

  async function send() {
    const prompt = input.value.trim();
    if (!prompt) return;
    addMsg("user", prompt);
    input.value = "";
    status.textContent = "Thinking...";

    let reply = "";
    try {
      if (MODE === "ollama") {
        const res = await fetch(OLLAMA_URL, {
          method: "POST",
          headers: {"Content-Type":"application/json"},
          body: JSON.stringify({
            model: "llama3.2:1b-instruct-q4_0",  // change if you pulled another
            messages: [{role:"user", content:prompt}],
            stream: false
          })
        });
        const data = await res.json();
        reply = data.message.content;
      } else {
        if (!webllmEngine) await initWebLLM();
        const out = await webllmEngine(prompt, { max_new_tokens: 256 });
        reply = out[0].generated_text.replace(prompt, "").trim();
      }
    } catch(e) {
      reply = "Error: " + e.message;
    }

    addMsg("ai", reply);
    status.textContent = MODE==="ollama"?"Ollama ready":"Browser ready";
  }

  // Auto-init WebLLM on first click if needed
  if (MODE!=="ollama") status.textContent = "Click Send to load model";
</script>
</body>
</html>
