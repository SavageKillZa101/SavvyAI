<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Low-End AI Chat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body{font-family: system-ui, sans-serif; max-width: 700px; margin: 2rem auto; padding: 1rem;}
    #chat{ border: 1px solid #ccc; height: 60vh; overflow-y: auto; padding: 1rem; margin-bottom: 1rem; background:#fafafa;}
    .msg{margin:0.5rem 0;}
    .user{color:#d00; font-weight:bold;}
    .ai{color:#00a;}
    textarea{width:100%; box-sizing:border-box; font-family:inherit;}
    button{margin-top:0.5rem; padding:0.5rem 1rem;}
    #status{font-size:0.8rem; color:#666;}
  </style>
</head>
<body>

<h1>Low-End AI Chat</h1>
<div id="chat"></div>
<textarea id="input" rows="3" placeholder="Ask anything..."></textarea><br/>
<button onclick="send()">Send</button>
<div id="status">Choose mode →</div>

<script>
  // ==== CONFIG ==== 
  const MODE = "webllm";   // Change to "webllm" for browser-only (no install)

  // Ollama (change only if you run on non-standard port)
  const OLLAMA_URL = "http://localhost:11434/api/chat";

  // WebLLM model (hosted on HF – no key needed)
  const WEBLLM_MODEL = "onnx-community/Llama-3.2-1B-Instruct-q4f16";

  // ==== END CONFIG ====

  const chat = document.getElementById("chat");
  const input = document.getElementById("input");
  const status = document.getElementById("status");

  let webllmEngine = null;

  async function initWebLLM() {
    status.textContent = "Downloading model (~400 MB)...";
    try {
      const { pipeline } = await import("https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.4.0/+esm");
      webllmEngine = await pipeline('text-generation', WEBLLM_MODEL, { 
        quantized: true,
        dtype: 'q4f16'
      });
      status.textContent = "Model ready (browser mode)";
    } catch (e) {
      status.textContent = "Browser mode error: " + e.message + " (Try Chrome/Edge)";
    }
  }

  function addMsg(role, text) {
    const div = document.createElement("div");
    div.className = "msg " + role;
    div.textContent = (role==="user"?"You":"AI") + ": " + text;
    chat.appendChild(div);
    chat.scrollTop = chat.scrollHeight;
  }

  async function send() {
    const prompt = input.value.trim();
    if (!prompt) return;
    addMsg("user", prompt);
    input.value = "";
    status.textContent = "Thinking...";

    let reply = "";
    try {
      if (MODE === "ollama") {
        const res = await fetch(OLLAMA_URL, {
          method: "POST",
          headers: {"Content-Type":"application/json"},
          body: JSON.stringify({
            model: "phi3:mini-4k-instruct-q4",  // Tiny & fast – swap as needed
            messages: [{role:"user", content:prompt}],
            stream: false,
            options: { temperature: 0.7 }
          })
        });
        if (!res.ok) throw new Error(`Ollama error: ${res.status}`);
        const data = await res.json();
        reply = data.message.content;
      } else {
        if (!webllmEngine) await initWebLLM();
        if (!webllmEngine) throw new Error("Model failed to load");
        const out = await webllmEngine(prompt, { max_new_tokens: 256, temperature: 0.7 });
        reply = out[0].generated_text.replace(prompt, "").trim();
      }
    } catch(e) {
      reply = "Error: " + e.message + "\n\nTip: For Ollama, run 'ollama serve' in terminal.\nFor browser, switch MODE='webllm' and retry.";
    }

    addMsg("ai", reply);
    status.textContent = MODE==="ollama"?"Ollama ready":"Browser ready";
  }

  // Enter key to send
  input.addEventListener("keydown", (e) => { if (e.key==="Enter" && e.ctrlKey) send(); });

  // Auto-init message
  if (MODE!=="ollama") status.textContent = "Click Send to load browser model (first time only)";
</script>
</body>
</html>

